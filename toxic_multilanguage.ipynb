{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "toxic-multilanguage.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1vxqgzoQ3GAvlMnG3Gek5GkWZLjUgvAGb",
      "authorship_tag": "ABX9TyN8r+j2/8Vbr01dZ0itNa/m"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUxqdL-C4dwP"
      },
      "source": [
        "!pip install transformers \n",
        "!pip install comet-ml\n",
        "!pip install pytorch-lightning\n",
        "!git clone https://github.com/bohdan1/AbusiveLanguageDataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "es6Qnekf49yj"
      },
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n",
        "import os\n",
        "import torch \n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import pytorch_lightning as pl\n",
        "from torch.utils.data import random_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from transformers import BertForSequenceClassification, BertTokenizerFast, Trainer, TrainingArguments\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7RB5eyi5DJp"
      },
      "source": [
        "data_path = './AbusiveLanguageDataset/'\n",
        "data = pd.read_csv(os.path.join(data_path,'data.csv'))\n",
        "labeled = pd.read_csv(os.path.join(data_path,'labled.csv'))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTPSNY2PkOxy",
        "outputId": "3be47a79-c4ce-4f98-88ce-5c97bc5b5f2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>video_id</th>\n",
              "      <th>author</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>UgygtiZyJIQoKYMr5-14AaABAg</td>\n",
              "      <td>_k-gDXrfu-s</td>\n",
              "      <td>Taki Mizu</td>\n",
              "      <td>Нужно было из града всю толпу поливать﻿</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>UgxdRnsYt37Ega0lMfJ4AaABAg</td>\n",
              "      <td>_k-gDXrfu-s</td>\n",
              "      <td>Дональд Трамп</td>\n",
              "      <td>Слава Беркуту,спасибо зато, что хотели уберечь...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>UgyM1RlUb-cSyMCfGTt4AaABAg</td>\n",
              "      <td>_k-gDXrfu-s</td>\n",
              "      <td>Светлана Агзамова</td>\n",
              "      <td>беркуту позор﻿</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Ugy_jfinirWXeqK3f5p4AaABAg</td>\n",
              "      <td>_k-gDXrfu-s</td>\n",
              "      <td>Sonya Nishpal</td>\n",
              "      <td>как можно быть такими жестокими?(! извиняюсь з...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Ugx85dWKg6e-RcpEuv94AaABAg</td>\n",
              "      <td>_k-gDXrfu-s</td>\n",
              "      <td>Bro Rik</td>\n",
              "      <td>в итоге продали свою жопу) за что дохли как со...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                           id  ...                                               text\n",
              "0  UgygtiZyJIQoKYMr5-14AaABAg  ...            Нужно было из града всю толпу поливать﻿\n",
              "1  UgxdRnsYt37Ega0lMfJ4AaABAg  ...  Слава Беркуту,спасибо зато, что хотели уберечь...\n",
              "2  UgyM1RlUb-cSyMCfGTt4AaABAg  ...                                     беркуту позор﻿\n",
              "3  Ugy_jfinirWXeqK3f5p4AaABAg  ...  как можно быть такими жестокими?(! извиняюсь з...\n",
              "4  Ugx85dWKg6e-RcpEuv94AaABAg  ...  в итоге продали свою жопу) за что дохли как со...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1T0rM4qP5Yl1"
      },
      "source": [
        "labels = labeled['text']\n",
        "targets = labeled['abusive']"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iP6dJnoM7SOu"
      },
      "source": [
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    labels, targets, test_size=0.2, random_state=42)\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whmpMg65hke2",
        "outputId": "d5d65e0f-405f-4e2b-dc3a-7b709dff3e1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig, BertTokenizer\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"DeepPavlov/bert-base-bg-cs-pl-ru-cased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")# Tell pytorch to run this model on the GPU.\n",
        "tokenizer = BertTokenizer.from_pretrained(\"DeepPavlov/bert-base-bg-cs-pl-ru-cased\", max_length = 512)\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/bert-base-bg-cs-pl-ru-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3x6wvMXSLWm"
      },
      "source": [
        "train_encodings = (tokenizer(list(train_texts), truncation=True, padding=True,max_length = 512))\n",
        "val_encodings = (tokenizer(list(val_texts), truncation=True, padding=True,max_length = 512))\n",
        "test_encodings = (tokenizer(list(test_texts), truncation=True, padding=True,max_length = 512))\n",
        "\n",
        "train_labels = list(train_labels*1)\n",
        "test_labels = list(test_labels*1)\n",
        "val_labels = list(val_labels*1)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nShtma93WbFU"
      },
      "source": [
        "class ToxDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = ToxDataset(train_encodings, train_labels)\n",
        "val_dataset = ToxDataset(val_encodings, val_labels)\n",
        "test_dataset = ToxDataset(test_encodings, test_labels)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49198klC8Puy"
      },
      "source": [
        "def save_model(save_path  = 'saved_model'):\n",
        "  model.save_pretrained(save_path)\n",
        "  tokenizer.save_pretrained(save_path)\n",
        "save_model()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvTpHFpnQNRd",
        "outputId": "b2b901d5-b523-4556-8e53-7d62fcdd56e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=16,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    evaluate_during_training=True,\n",
        "    logging_dir='./logs',\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset\n",
        ")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DeNUpP2TCOC",
        "outputId": "0ce6f4fd-0366-4b58-9bd4-daf9e5a92c37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "trainer.evaluate()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [20/20 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'epoch': 5.0,\n",
              " 'eval_accuracy': 0.74375,\n",
              " 'eval_f1': 0.6132075471698113,\n",
              " 'eval_loss': 1.5390530824661255,\n",
              " 'eval_precision': 0.6190476190476191,\n",
              " 'eval_recall': 0.6074766355140186,\n",
              " 'total_flos': 3496771151462400}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    }
  ]
}